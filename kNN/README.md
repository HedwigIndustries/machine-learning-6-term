# Лабораторная работа №1: **Метод ближайших соседей**

## Алгоритм

### Реализация метода:
1. Алгоритм должен работать с **окнами фиксированного и нефиксированного размера**.
2. Алгоритм должен поддерживать **различные ядра**:
   - Необходимо реализовать **не менее 4 ядер**.
   - Обязательные: **Равномерное** и **Гауссово**.
   - Желательно реализовать **общее ядро** вида ![equation](https://latex.codecogs.com/svg.latex?\left(1-%7Cu%7C^a\right)^b).
3. Поддержка **различных метрик**:
   - **Не менее 3 метрик**.
   - Обязательное: **Косинусное расстояние**.
   - Желательно: расстояние **Минковского (Lp)**.
4. Алгоритм должен работать с **априорными весами** объектов.
5. **Использование готовых библиотек** разрешено для поиска ближайших объектов.

### Библиотечная реализация
Выберите библиотечную реализацию метода ближайших соседей, например, из [scikit-learn](https://scikit-learn.org/stable/modules/neighbors.html).

---

## Набор данных

1. Выберите любой набор данных для задачи **классификации**. Желательно использовать данные из курса **Анализа данных**.
   - Примеры данных можно найти на платформах, таких как [Kaggle](https://www.kaggle.com/datasets) или [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).
   
2. **Преобразуйте данные в числовой вид** и **нормализуйте** их.

3. Разделите данные на **тренировочную** и **тестовую** выборки.

4. Выберите целевую функцию для **оценки качества** модели, например, **логистическую функцию потерь**, **accuracy** или **f1-score**.

---

## Гиперпараметры

### Настройка гиперпараметров:

1. Настройте гиперпараметры как для **реализованного алгоритма**, так и для библиотечного:
   - Для поиска гиперпараметров рекомендуется использовать библиотеку [Optuna](https://optuna.org/), которая поддерживает мощные алгоритмы оптимизации.

2. В процессе настройки **не используйте тестовое множество** для валидации.

3. Выведите **лучшие значения гиперпараметров** для обоих алгоритмов.

### Визуализация:

- Постройте графики зависимости функции ошибки/качества на **тестовом** и **тренировочном** множествах от числа соседей или ширины окна (с зафиксированными остальными параметрами).

---

## Поиск аномалий

1. Реализуйте алгоритм **поиска аномалий с использованием метода LOWESS** (Locally Weighted Scatterplot Smoothing). Узнать больше о методе можно в [статье на Wikipedia](https://en.wikipedia.org/wiki/Local_regression).

2. **Взвесьте объекты** из тренировочного множества перед применением KNN.

3. Проверьте результаты **валидации** реализованного алгоритма на тестовом множестве **до и после взвешивания объектов**.

4. Повторите эксперимент с библиотечным KNN:
   - Если библиотека не поддерживает **априорные веса**, примените **семплирование** для создания аналогичного эффекта.

---

## Дополнительные материалы

- Документация по KNN в [scikit-learn](https://scikit-learn.org/stable/modules/neighbors.html).
- Гипероптимизация с [Optuna](https://optuna.org/).
- Теория метода LOWESS на [Wikipedia](https://en.wikipedia.org/wiki/Local_regression).
