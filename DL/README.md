# Лабораторная работа: **Глубокое обучение**

## Набор данных

1. **Выберите набор данных** для задачи классификации.
   - Желательно использовать набор данных с курса **Анализа данных**.
   - Примеры наборов данных: [Kaggle Datasets](https://www.kaggle.com/datasets) или [UCI Repository](https://archive.ics.uci.edu/ml/index.php).

2. **Преобразуйте данные в числовой вид**.

3. Разбейте данные на **тренировочную** и **тестовую части**.

4. Выберите **целевую функцию ошибки** или **метрику качества**. Например, **cross-entropy** или **accuracy**.

---

## Задание

### 1. Выбор фреймворка
- Выберите фреймворк для глубокого обучения:
  - **TensorFlow/Keras**: [Keras Documentation](https://keras.io/).
  - **PyTorch**: [PyTorch Documentation](https://pytorch.org/).

### 2. Многослойный перцептрон (MLP)
- **Реализуйте многослойную архитектуру** (MLP), где:
  - **Число слоёв** является гиперпараметром.
  - Пример структуры: [MLP in PyTorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).

### 3. Архитектура с остаточными связями (ResNet)
- **Реализуйте архитектуру с остаточными связями** (ResNet):
  - **Число слоёв** также является гиперпараметром.
  - Полезные материалы: [ResNet in PyTorch](https://pytorch.org/vision/stable/models.html#id2).

### 4. График зависимости качества от числа слоёв
- Постройте **график зависимости** целевой функции на тренировочной и тестовой части в зависимости от числа слоёв для обеих архитектур (MLP и ResNet).

### 5. Инициализация параметров
- **Повторите пункт 4**, используя инициализацию:
  - **Xavier**: [Xavier Initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_).
  - **He**: [He Initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_).

### 6. Пакетная нормализация
- **Добавьте пакетную нормализацию** в архитектуры.
  - Полезные материалы: [BatchNorm in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html).
  
- Повторите пункт 4 для этих модифицированных архитектур.

### 7. Оптимальная модификация ResNet
- Найдите **оптимальную модификацию ResNet** с **оптимальным числом слоёв**.

### 8. Глубокая ResNet архитектура
- Выберите **глубокую ResNet архитектуру** с большим числом слоёв, которая всё ещё способна обучаться.

### 9. Оценка слоёв
- Для архитектуры из предыдущего пункта **вычислите среднее значение** выхода каждого слоя с остаточными связями.
- **Удалите слои с наименьшими значениями**, не перезаписывая параметры оставшихся слоёв.
- Протестируйте полученную архитектуру.

### 10. Дообучение и сравнение
- **Дообучите** архитектуру из предыдущего пункта и постройте кривую обучения.
- **Обучите аналогичную архитектуру**, но без переиспользования параметров (инициализируйте их заново), и постройте кривую обучения для неё.

---

## Дополнительные материалы

- [ResNet Architecture](https://arxiv.org/abs/1512.03385)
- [Batch Normalization](https://arxiv.org/abs/1502.03167)
- [Xavier Initialization](https://proceedings.mlr.press/v9/glorot10a.html)
- [He Initialization](https://arxiv.org/abs/1502.01852)
